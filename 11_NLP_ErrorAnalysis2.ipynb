{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis (2)\n",
    "\n",
    "From the previous notebook, you have seen that pyConTextNLP helped to improve the precision by excluding irrelevant annotations based on the modifiers. \n",
    "\n",
    "This notebook will continue our previous error analyses by including both false negative errors and false positive ones, through step by step demonstration of how to locate and reduce the errors.\n",
    "\n",
    "## 1. Locate the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some packages\n",
    "import os\n",
    "import pyConTextNLP\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import radnlp.view as rview\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, HTML, Image\n",
    "import ipywidgets\n",
    "# And also our utilities for this class\n",
    "\n",
    "from nlp_pneumonia_utils import read_doc_annotations\n",
    "from nlp_pneumonia_utils import mark_document_with_html\n",
    "from DocumentClassifier import DocumentClassifier\n",
    "from visual import view_pycontext_output\n",
    "from visual import snippets_markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember yesterday, in [06_NLP_ErrorAnalysis1](06_NLP_ErrorAnalysis1.ipynb#cell1), we created a function called *\"list_false_negatives.\"* Now we will extend this function to return both **false negative** and **false positive** document names at the same time: *\"list_errors.\"* Additionally, we will also integrate the *\"calculate_prediction_metrics\"* function inside *\"list_errors\"*, so that we can get the metrics without re-run the pyConText over the documents again.\n",
    "<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_errors(gold_docs, prediction_function,\n",
    "                print_prediction_metrics=False):\n",
    "    fn_docs = []\n",
    "    fp_docs = []\n",
    "    gold_labels = [x.positive_label for x in gold_docs.values()]\n",
    "    pred_labels = []\n",
    "    for doc_name, gold_doc in gold_docs.items():\n",
    "        gold_label = gold_doc.positive_label;\n",
    "        pred_label = prediction_function(gold_doc.text, doc_name)\n",
    "        pred_labels.append(pred_label)\n",
    "        #       Differentiate false positive and false negative error\n",
    "        if gold_label == 0 and pred_label == 1:\n",
    "            fp_docs.append(doc_name)\n",
    "        elif gold_label == 1 and pred_label == 0:\n",
    "            fn_docs.append(doc_name)\n",
    "    if (print_prediction_metrics):\n",
    "        precision = sklearn.metrics.precision_score(gold_labels, pred_labels)\n",
    "        recall = sklearn.metrics.recall_score(gold_labels, pred_labels)\n",
    "        f1 = sklearn.metrics.f1_score(gold_labels, pred_labels)\n",
    "        # Let's use Pandas to make a confusion matrix for us\n",
    "        se1 = pd.Series(gold_labels, name='Actual');\n",
    "        se2 = pd.Series(pred_labels, name='Predicted')\n",
    "        confusion_matrix_df = pd.crosstab(pd.Series(gold_labels, name='Actual'),\n",
    "                                          pd.Series(pred_labels, name='Predicted'))\n",
    "\n",
    "        print('Precision : {0}'.format(precision))\n",
    "        print('Recall :    {0}'.format(recall))\n",
    "        print('F1:         {0}'.format(f1))\n",
    "\n",
    "        print('\\nConfusion Matrix : ')\n",
    "        print(confusion_matrix_df)\n",
    "    return fn_docs, fp_docs  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restore what we got from [10_NLP_DocumentClassification.ipynb](10_NLP_DocumentClassification.ipynb):<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the training documents and annotations\n",
    "annotated_doc_map = read_doc_annotations('data/training_v2.zip')\n",
    "\n",
    "#Here we initiate our DocumentClassifier directly through rule files:\n",
    "#Change the file names if you use different files \n",
    "pos_doc_type='PNEUMONIA_DOC_YES'\n",
    "TARGETS_FILE_PATH = 'KB/pneumonia_targets.tsv'\n",
    "MODIFIERS_FILE_PATH = 'KB/pneumonia_modifiers.tsv'\n",
    "FEATURE_INFERENCER_FILE_PATH = 'KB/featurer_inferences.csv'\n",
    "DOC_INFERENCER_FILE_PATH = 'KB/doc_inferences.csv'\n",
    "# clear just in case files/regular expressions have been updated\n",
    "classifier = DocumentClassifier(TARGETS_FILE_PATH, MODIFIERS_FILE_PATH,\n",
    "                               FEATURE_INFERENCER_FILE_PATH, DOC_INFERENCER_FILE_PATH,\n",
    "                               {pos_doc_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the corpus using docClassifier to return errors\n",
    "current_false_negatives,current_false_positives=list_errors(annotated_doc_map, classifier.predict,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocumentClassifier also has a wrapped-up function to do this evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_false_negatives, current_false_positives, measurements,confusion_matrix_df = classifier.eval(annotated_doc_map)\n",
    "print(measurements)\n",
    "display(confusion_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display errors\n",
    "Now we put everything together to display errors. Let's try false negative first:<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Display false negatives\n",
    "Now we can display the **false negatives** with expert annotations.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_docs=dict((k, v) for k, v in annotated_doc_map.items() if k in current_false_negatives)\n",
    "display(HTML(snippets_markup(fn_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Display false positives\n",
    "Then we can display the **false positives** with pyConText markups.<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_docs = dict((k,v) for k, v in classifier.saved_markups_map.items() if k in current_false_positives)\n",
    "view_pycontext_output(fp_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Rethink the causes of the false negatives\n",
    "Does the false negatives are all caused by missed keywords?<br/><br/>\n",
    "You may want to review these pyConText markups in **false negative** documents:<br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_docs = dict((k,v) for k, v in classifier.saved_markups_map.items() if k in current_false_negatives)\n",
    "view_pycontext_output(fn_docs)\n",
    "# The variable: \"saved_markups_map\" in \"docClassifier\" only saves the documents that have at least one annotation. \n",
    "# Thus, the false negatives caused by missing keywords (no annotations) will not be saved in it,\n",
    "# and only pyConText caused false negatives will be displayed here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now what?<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quiz\n",
    "Let's see if you are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_utils import error_analyses_7\n",
    "error_analyses_7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_utils import error_analyses_8\n",
    "error_analyses_8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quiz_utils import error_analyses_9\n",
    "error_analyses_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>This material presented as part of the DeCART Data Science for the Health Science Summer Program at the University of Utah in 2018.<br/>\n",
    "Presenters : Dr.Wendy Chapman, Jianlin Shi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
